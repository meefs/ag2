<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A programming framework for agentic AI"><meta name=author content="Chi Wang & Qingyun Wu"><link href=http://docs.ag2.ai/latest/docs/blog/category/llm/page/2/ rel=canonical><link rel=icon href=../../../../../../assets/img/favicon.svg><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.9"><title>LLM - AG2</title><link rel=stylesheet href=../../../../../../assets/stylesheets/main.4af4bdda.min.css><link rel=stylesheet href=../../../../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Inter";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../../../../css/timeago.css><link rel=stylesheet href=../../../../../../assets/_mkdocstrings.css><link rel=stylesheet href=../../../../../../stylesheets/extra.aa8cf5.min.css><script>__md_scope=new URL("../../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-2GN2KN2CE4"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-2GN2KN2CE4",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-2GN2KN2CE4",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta http-equiv=Cache-Control content="no-cache, no-store, must-revalidate"><meta property=og:type content=website><meta property=og:title content="AG2 - LLM"><meta property=og:description content="A programming framework for agentic AI"><meta content=http://docs.ag2.ai/latest/docs/blog/category/llm/page/2/ property=og:url><meta property=og:image content=https://opengraph.githubassets.com/1671805243.560327/ag2ai/ag2><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="AG2 - LLM"><meta name=twitter:description content="A programming framework for agentic AI"><meta name=twitter:image content=https://opengraph.githubassets.com/1671805243.560327/ag2ai/ag2><link href=../../../../../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../../../../../assets/javascripts/glightbox.min.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=custom data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#llm class=md-skip> Skip to content </a> </div> <div data-md-component=announce> <aside class=md-banner> <div class="md-banner__inner md-grid md-typeset"> You're viewing the <strong class=primary-text>work in progress</strong> version of the documentation. Please click <a rel=me href=http://docs.ag2.ai><strong>here</strong></a> for the latest. </div> </aside> </div> <div data-md-color-scheme=default data-md-component=outdated hidden> <aside class="md-banner md-banner--warning"> <div class="md-banner__inner md-grid md-typeset"> You're not viewing the latest version. <a href=../../../../../../..> <strong>Click here to go to latest.</strong> </a> </div> <script>var el=document.querySelector("[data-md-component=outdated]"),base=new URL("../../../../../.."),outdated=__md_get("__outdated",sessionStorage,base);!0===outdated&&el&&(el.hidden=!1)</script> </aside> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../../../../.. title=AG2 class="md-header__button md-logo" aria-label=AG2 data-md-component=logo> <img src=../../../../../../assets/img/logo.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AG2 </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> LLM </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=custom data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=custom data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/ag2ai/ag2 title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> ag2ai/ag2 </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../../../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../../../user-guide/basic-concepts/installing-ag2/ class=md-tabs__link> User Guide </a> </li> <li class=md-tabs__item> <a href=../../../../../api-reference/autogen/AfterWork/ class=md-tabs__link> API References </a> </li> <li class=md-tabs__item> <a href=../../../../../contributor-guide/contributing/ class=md-tabs__link> Contributor Guide </a> </li> <li class=md-tabs__item> <a href=../../../../../faq/FAQ/ class=md-tabs__link> FAQs </a> </li> <li class=md-tabs__item> <a href=../../../../../ecosystem/agentops/ class=md-tabs__link> Ecosystem </a> </li> <li class=md-tabs__item> <a href=../../../.. class=md-tabs__link> Blog </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../../../../.. title=AG2 class="md-nav__button md-logo" aria-label=AG2 data-md-component=logo> <img src=../../../../../../assets/img/logo.svg alt=logo> </a> AG2 </label> <div class=md-nav__source> <a href=https://github.com/ag2ai/ag2 title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> ag2ai/ag2 </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../user-guide/basic-concepts/installing-ag2/ class=md-nav__link> <span class=md-ellipsis> User Guide </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../api-reference/autogen/AfterWork/ class=md-nav__link> <span class=md-ellipsis> API References </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../contributor-guide/contributing/ class=md-nav__link> <span class=md-ellipsis> Contributor Guide </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../faq/FAQ/ class=md-nav__link> <span class=md-ellipsis> FAQs </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../../../ecosystem/agentops/ class=md-nav__link> <span class=md-ellipsis> Ecosystem </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../../.. class=md-nav__link> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#agent-autobuild-automatically-building-multi-agent-systems class=md-nav__link> <span class=md-ellipsis> Agent AutoBuild - Automatically Building Multi-agent Systems </span> </a> </li> <li class=md-nav__item> <a href=#how-to-assess-utility-of-llm-powered-applications class=md-nav__link> <span class=md-ellipsis> How to Assess Utility of LLM-powered Applications? </span> </a> </li> <li class=md-nav__item> <a href=#ecoassistant-using-llm-assistants-more-accurately-and-affordably class=md-nav__link> <span class=md-ellipsis> EcoAssistant - Using LLM Assistants More Accurately and Affordably </span> </a> </li> <li class=md-nav__item> <a href=#autogens-teachable-agents class=md-nav__link> <span class=md-ellipsis> AutoGen's Teachable Agents </span> </a> </li> <li class=md-nav__item> <a href=#retrieval-augmented-generation-rag-applications-with-autogen class=md-nav__link> <span class=md-ellipsis> Retrieval-Augmented Generation (RAG) Applications with AutoGen </span> </a> </li> <li class=md-nav__item> <a href=#use-autogen-for-local-llms class=md-nav__link> <span class=md-ellipsis> Use AutoGen for Local LLMs </span> </a> </li> <li class=md-nav__item> <a href=#mathchat-an-conversational-framework-to-solve-math-problems class=md-nav__link> <span class=md-ellipsis> MathChat - An Conversational Framework to Solve Math Problems </span> </a> </li> <li class=md-nav__item> <a href=#achieve-more-pay-less-use-gpt-4-smartly class=md-nav__link> <span class=md-ellipsis> Achieve More, Pay Less - Use GPT-4 Smartly </span> </a> </li> <li class=md-nav__item> <a href=#does-model-and-inference-parameter-matter-in-llm-applications-a-case-study-for-math class=md-nav__link> <span class=md-ellipsis> Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <div class=md-content__inner> <header class=md-typeset> <h1 id=llm>LLM<a class=headerlink href=#llm title="Permanent link">#</a></h1> </header> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://github.com/LinxinS97.png alt="Linxin Song"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2023-11-26 00:00:00+00:00">November 26, 2023</time></li> <li class=md-meta__item> in <a href=../../ class=md-meta__link>LLM</a>, <a href=../../../research/ class=md-meta__link>research</a></li> <li class=md-meta__item> 6 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=agent-autobuild-automatically-building-multi-agent-systems><a href=../../../../2023/11/26/Agent-AutoBuild/ class=toclink>Agent AutoBuild - Automatically Building Multi-agent Systems</a></h2> <p><img alt="Overall structure of AutoBuild" src=../../../../2023-11-26-Agent-AutoBuild/img/agent_autobuild.webp></p> <p><strong>TL;DR:</strong> Introducing <strong>AutoBuild</strong>, building multi-agent system automatically, fast, and easily for complex tasks with minimal user prompt required, powered by a new designed class <strong>AgentBuilder</strong>. AgentBuilder also supports open-source LLMs by leveraging <a href=https://docs.vllm.ai/en/latest/index.html>vLLM</a> and <a href=https://github.com/lm-sys/FastChat>FastChat</a>. Checkout example notebooks and source code for reference:</p> <ul> <li><a href=https://github.com/ag2ai/ag2/blob/main/notebook/autobuild_basic.ipynb>AutoBuild Examples</a></li> <li><a href=https://github.com/ag2ai/ag2/blob/main/autogen/agentchat/contrib/captainagent/agent_builder.py>AgentBuilder</a></li> </ul> <h3 id=introduction><a class=toclink href=../../../../2023/11/26/Agent-AutoBuild/#introduction>Introduction</a></h3> <p>In this blog, we introduce <strong>AutoBuild</strong>, a pipeline that can automatically build multi-agent systems for complex tasks. Specifically, we design a new class called <strong>AgentBuilder</strong>, which will complete the generation of participant expert agents and the construction of group chat automatically after the user provides descriptions of a building task and an execution task.</p> <p>AgentBuilder supports open-source models on Hugging Face powered by <a href=https://docs.vllm.ai/en/latest/index.html>vLLM</a> and <a href=https://github.com/lm-sys/FastChat>FastChat</a>. Once the user chooses to use open-source LLM, AgentBuilder will set up an endpoint server automatically without any user participation.</p> <nav class=md-post__action> <a href=../../../../2023/11/26/Agent-AutoBuild/ > Continue reading </a> </nav> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src="https://avatars.githubusercontent.com/u/5908392?v=4" alt="Julia Kiseleva"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2023-11-20 00:00:00+00:00">November 20, 2023</time></li> <li class=md-meta__item> in <a href=../../ class=md-meta__link>LLM</a>, <a href=../../../gpt/ class=md-meta__link>GPT</a>, <a href=../../../evaluation/ class=md-meta__link>evaluation</a>, <a href=../../../task-utility/ class=md-meta__link>task utility</a></li> <li class=md-meta__item> 8 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=how-to-assess-utility-of-llm-powered-applications><a href=../../../../2023/11/20/AgentEval/ class=toclink>How to Assess Utility of LLM-powered Applications?</a></h2> <p><img alt="Fig.1: A verification framework" src=../../../../2023-11-20-AgentEval/img/agenteval-CQ.webp></p> <p align=center><em>Fig.1 illustrates the general flow of AgentEval</em></p> <p><strong>TL;DR:</strong> * As a developer of an LLM-powered application, how can you assess the utility it brings to end users while helping them with their tasks? * To shed light on the question above, we introduce <code>AgentEval</code> — the first version of the framework to assess the utility of any LLM-powered application crafted to assist users in specific tasks. AgentEval aims to simplify the evaluation process by automatically proposing a set of criteria tailored to the unique purpose of your application. This allows for a comprehensive assessment, quantifying the utility of your application against the suggested criteria. * We demonstrate how <code>AgentEval</code> work using <a href=https://docs.ag2.ai/blog/2023-06-28-MathChat>math problems dataset</a> as an example in the <a href=https://github.com/ag2ai/ag2/blob/main/notebook/agenteval_cq_math.ipynb>following notebook</a>. Any feedback would be useful for future development. Please contact us on our <a href=https://discord.gg/pAbnFJrkgZ>Discord</a>.</p> <h3 id=introduction><a class=toclink href=../../../../2023/11/20/AgentEval/#introduction>Introduction</a></h3> <p>AutoGen aims to simplify the development of LLM-powered multi-agent systems for various applications, ultimately making end users' lives easier by assisting with their tasks. Next, we all yearn to understand how our developed systems perform, their utility for users, and, perhaps most crucially, how we can enhance them. Directly evaluating multi-agent systems poses challenges as current approaches predominantly rely on success metrics – essentially, whether the agent accomplishes tasks. However, comprehending user interaction with a system involves far more than success alone. Take math problems, for instance; it's not merely about the agent solving the problem. Equally significant is its ability to convey solutions based on various criteria, including completeness, conciseness, and the clarity of the provided explanation. Furthermore, success isn't always clearly defined for every task.</p> <p>Rapid advances in LLMs and multi-agent systems have brought forth many emerging capabilities that we're keen on translating into tangible utilities for end users. We introduce the first version of <code>AgentEval</code> framework - a tool crafted to empower developers in swiftly gauging the utility of LLM-powered applications designed to help end users accomplish the desired task.</p> <p><img alt="Fig.2: An overview of the tasks taxonomy" src=../../../../2023-11-20-AgentEval/img/tasks-taxonomy.webp></p> <p align=center><em>Fig. 2 provides an overview of the tasks taxonomy</em></p> <p>Let's first look into an overview of the suggested task taxonomy that a multi-agent system can be designed for. In general, the tasks can be split into two types, where: * <em>Success is not clearly defined</em> - refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex. * <em>Success is clearly defined</em> - refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories: * <em>The optimal solution exits</em> - these are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it. * <em>Multiple solutions exist</em> - increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.</p> <p>In our <code>AgentEval</code> framework, we are currently focusing on tasks where <em>Success is clearly defined</em>. Next, we will introduce the suggested framework.</p> <nav class=md-post__action> <a href=../../../../2023/11/20/AgentEval/ > Continue reading </a> </nav> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://github.com/jieyuz2.png alt="Jieyu Zhang"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2023-11-09 00:00:00+00:00">November 9, 2023</time></li> <li class=md-meta__item> in <a href=../../ class=md-meta__link>LLM</a>, <a href=../../../rag/ class=md-meta__link>RAG</a>, <a href=../../../cost-effectiveness/ class=md-meta__link>cost-effectiveness</a></li> <li class=md-meta__item> 5 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=ecoassistant-using-llm-assistants-more-accurately-and-affordably><a href=../../../../2023/11/09/EcoAssistant/ class=toclink>EcoAssistant - Using LLM Assistants More Accurately and Affordably</a></h2> <p><img alt=system src=../../../../2023-11-09-EcoAssistant/img/system.webp></p> <p><strong>TL;DR:</strong> * Introducing the <strong>EcoAssistant</strong>, which is designed to solve user queries more accurately and affordably. * We show how to let the LLM assistant agent leverage external API to solve user query. * We show how to reduce the cost of using GPT models via <strong>Assistant Hierarchy</strong>. * We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via <strong>Solution Demonstration</strong>.</p> <h3 id=ecoassistant><a class=toclink href=../../../../2023/11/09/EcoAssistant/#ecoassistant>EcoAssistant</a></h3> <p>In this blog, we introduce the <strong>EcoAssistant</strong>, a system built upon AutoGen with the goal of solving user queries more accurately and affordably.</p> <nav class=md-post__action> <a href=../../../../2023/11/09/EcoAssistant/ > Continue reading </a> </nav> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://github.com/rickyloynd-microsoft.png alt="Ricky Loynd"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2023-10-26 00:00:00+00:00">October 26, 2023</time></li> <li class=md-meta__item> in <a href=../../ class=md-meta__link>LLM</a>, <a href=../../../teach/ class=md-meta__link>teach</a></li> <li class=md-meta__item> 13 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=autogens-teachable-agents><a href=../../../../2023/10/26/TeachableAgent/ class=toclink>AutoGen's Teachable Agents</a></h2> <p><img alt="Teachable Agent Architecture" src=../../../../2023-10-26-TeachableAgent/img/teachable-arch.png></p> <p><strong>TL;DR:</strong></p> <ul> <li>We introduce <strong>Teachable Agents</strong> so that users can teach their LLM-based assistants new facts, preferences, and skills.</li> <li>We showcase examples of teachable agents learning and later recalling facts, preferences, and skills in subsequent chats.</li> </ul> <h3 id=introduction><a class=toclink href=../../../../2023/10/26/TeachableAgent/#introduction>Introduction</a></h3> <p>Conversational assistants based on LLMs can remember the current chat with the user, and can also demonstrate in-context learning of user teachings during the conversation. But the assistant's memories and learnings are lost once the chat is over, or when a single chat grows too long for the LLM to handle effectively. Then in subsequent chats the user is forced to repeat any necessary instructions over and over.</p> <p><code>Teachability</code> addresses these limitations by persisting user teachings across chat boundaries in long-term memory implemented as a vector database. Instead of copying all of memory into the context window, which would eat up valuable space, individual memories (called memos) are retrieved into context as needed. This allows the user to teach frequently used facts and skills to the teachable agent just once, and have it recall them in later chats.</p> <p>Any instantiated <code>agent</code> that inherits from <code>ConversableAgent</code> can be made teachable by instantiating a <code>Teachability</code> object and calling its <code>add_to_agent(agent)</code> method. In order to make effective decisions about memo storage and retrieval, the <code>Teachability</code> object calls an instance of <code>TextAnalyzerAgent</code> (another AutoGen agent) to identify and reformulate text as needed for remembering facts, preferences, and skills. Note that this adds extra LLM calls involving a relatively small number of tokens, which can add a few seconds to the time a user waits for each response.</p> <nav class=md-post__action> <a href=../../../../2023/10/26/TeachableAgent/ > Continue reading </a> </nav> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://github.com/thinkall.png alt="Li Jiang"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2023-10-18 00:00:00+00:00">October 18, 2023</time></li> <li class=md-meta__item> in <a href=../../ class=md-meta__link>LLM</a>, <a href=../../../rag/ class=md-meta__link>RAG</a></li> <li class=md-meta__item> 8 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=retrieval-augmented-generation-rag-applications-with-autogen><a href=../../../../2023/10/18/RetrieveChat/ class=toclink>Retrieval-Augmented Generation (RAG) Applications with AutoGen</a></h2> <p><em>Last update: August 14, 2024; AutoGen version: v0.2.35</em></p> <p><img alt="RAG Architecture" src=../../../../2023-10-18-RetrieveChat/img/retrievechat-arch.png></p> <p><strong>TL;DR:</strong> * We introduce <strong>RetrieveUserProxyAgent</strong>, RAG agents of AutoGen that allows retrieval-augmented generation, and its basic usage. * We showcase customizations of RAG agents, such as customizing the embedding function, the text split function and vector database. * We also showcase two advanced usage of RAG agents, integrating with group chat and building a Chat application with Gradio.</p> <h3 id=introduction><a class=toclink href=../../../../2023/10/18/RetrieveChat/#introduction>Introduction</a></h3> <p>Retrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic limitations of LLMs by incorporating external documents. In this blog post, we introduce RAG agents of AutoGen that allows retrieval-augmented generation. The system consists of two agents: a Retrieval-augmented User Proxy agent, called <code>RetrieveUserProxyAgent</code>, and an Assistant agent, called <code>RetrieveAssistantAgent</code>; <code>RetrieveUserProxyAgent</code> is extended from built-in agents from AutoGen, while <code>RetrieveAssistantAgent</code> can be any conversable agent with LLM configured. The overall architecture of the RAG agents is shown in the figure above.</p> <p>To use Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented User Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy necessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented User Proxy can download the documents, segment them into chunks of a specific size, compute embeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively engage in code generation or question-answering adhering to the procedures outlined below: 1. The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding similarity, and sends them along with the question to the Retrieval-Augmented Assistant. 2. The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based on the question and context provided. If the LLM is unable to produce a satisfactory response, it is instructed to reply with “Update Context” to the Retrieval-Augmented User Proxy. 3. If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and sends the output as feedback. If there are no code blocks or instructions to update the context, it terminates the conversation. Otherwise, it updates the context and forwards the question along with the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation is enabled, individuals can proactively send any feedback, including Update Context”, to the Retrieval-Augmented Assistant. 4. If the Retrieval-Augmented Assistant receives “Update Context”, it requests the next most similar chunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it generates new code or text based on the feedback and chat history. If the LLM fails to generate an answer, it replies with “Update Context” again. This process can be repeated several times. The conversation terminates if no more documents are available for the context.</p> <nav class=md-post__action> <a href=../../../../2023/10/18/RetrieveChat/ > Continue reading </a> </nav> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://github.com/leoljl.png alt="Jiale Liu"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2023-07-14 00:00:00+00:00">July 14, 2023</time></li> <li class=md-meta__item> in <a href=../../ class=md-meta__link>LLM</a></li> <li class=md-meta__item> 2 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=use-autogen-for-local-llms><a href=../../../../2023/07/14/Local-LLMs/ class=toclink>Use AutoGen for Local LLMs</a></h2> <p><strong>TL;DR:</strong> We demonstrate how to use autogen for local LLM application. As an example, we will initiate an endpoint using <a href=https://github.com/lm-sys/FastChat>FastChat</a> and perform inference on <a href=https://github.com/THUDM/ChatGLM2-6B>ChatGLMv2-6b</a>.</p> <h3 id=preparations><a class=toclink href=../../../../2023/07/14/Local-LLMs/#preparations>Preparations</a></h3> <nav class=md-post__action> <a href=../../../../2023/07/14/Local-LLMs/ > Continue reading </a> </nav> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://github.com/yiranwu0.png alt="Yiran Wu"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2023-06-28 00:00:00+00:00">June 28, 2023</time></li> <li class=md-meta__item> in <a href=../../ class=md-meta__link>LLM</a>, <a href=../../../gpt/ class=md-meta__link>GPT</a>, <a href=../../../research/ class=md-meta__link>research</a></li> <li class=md-meta__item> 6 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=mathchat-an-conversational-framework-to-solve-math-problems><a href=../../../../2023/06/28/MathChat/ class=toclink>MathChat - An Conversational Framework to Solve Math Problems</a></h2> <p><img alt="MathChat WorkFlow" src=../../../../2023-06-28-MathChat/img/mathchatflow.png> <strong>TL;DR:</strong></p> <ul> <li><strong>We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.</strong></li> <li><strong>MathChat improves LLM's performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.</strong></li> <li><strong>Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.</strong></li> </ul> <p>Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated astonishing abilities over previous models on various tasks, such as text generation, question answering, and code generation. Moreover, these models can communicate with humans through conversations and remember previous contexts, making it easier for humans to interact with them. These models play an increasingly important role in our daily lives assisting people with different tasks, such as writing emails, summarizing documents, and writing code.</p> <p>In this blog post, we probe into the problem-solving capabilities of LLMs. Specifically, we are interested in their capabilities to solve advanced math problems, which could be representative of a broader class of problems that require precise reasoning and also have deterministic solutions.</p> <p>We introduce MathChat, a conversational framework designed for solving challenging math problems with LLMs. This framework takes advantage of the chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and an LLM assistant work together to tackle math problems. We also test previous prompting techniques for comparison.</p> <h3 id=the-mathchat-framework><a class=toclink href=../../../../2023/06/28/MathChat/#the-mathchat-framework>The MathChat Framework</a></h3> <p>MathChat simulates a conversation between the LLM assistant and a user proxy agent. As the name indicates, the user proxy agent acts as a proxy for the user, which is responsible for communicating with the LLM assistant and continuing the conversation in a desired manner.</p> <p>The proxy agent first presents a math problem to the LLM assistant, framed by an initial prompt that guides the assistant in how it should work collaboratively to solve the problem. We incorporate existing prompting methods such as CoT (chain-of-thought) and tool-using are incorporated in the initial prompt. Key components of the prompt include:</p> <ul> <li> <p><strong>Tool-using Prompt:</strong> This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.</p> </li> <li> <p><strong>Problem-Solving Strategy Selection Prompt:</strong> The assistant is instructed to choose one of three potential problem-solving strategies, including:</p> </li> <li> <p>Writing a Python program to solve the problem directly.</p> </li> <li>Solving the problem directly without using Python, which exercises LLMs' inherent reasoning capabilities.</li> <li> <p>Solving the problem step by step with Python. This should be used when the first two strategies aren't suitable.</p> </li> <li> <p><strong>Final Answer Encapsulation Prompt:</strong> This part instructs the assistant to put the final answer in <em>\boxed</em>{}.</p> </li> </ul> <p>The prompt design in MathChat is flexible and can easily be refined to include other tools, such as Wolfram Alpha.</p> <p>Let's take a look at an example between the <strong>User Proxy Agent</strong> and the <strong>LLM Assistant</strong> (GPT-4). The conversation focuses on how to solve inequality using Python. (The conversation is modified for readability.)</p> <table> <thead> <tr> <th>Participant</th> <th>Dialogue</th> </tr> </thead> <tbody> <tr> <td>User Proxy Agent</td> <td>(Initial Prompt: Let's solve a math problem with Python...) Problem: Find all x that satisfy the inequality (2x+10)(x+3) &lt; (3x+9)(x+8). Express your answer in interval notation.</td> </tr> <tr> <td>LLM assistant</td> <td>The key idea to solve this problem is to find the critical points, which are the points where the inequality is equal. Then, test the intervals between the critical points to find the interval for which the inequality holds.</td> </tr> <tr> <td>LLM assistant (continued)</td> <td>Here's Python code to find the critical points: <code>from sympy import symbols, Eq, solve; x = symbols("x"); lhs = (2*x + 10)*(x + 3); rhs = (3*x + 9)*(x + 8); equation = Eq(lhs, rhs); critical_points = solve(equation, x); critical_points</code></td> </tr> <tr> <td>User Proxy Agent</td> <td>The critical points are: [-14, -3]</td> </tr> <tr> <td>LLM assistant</td> <td>We found the critical points to be x = -14 and x = -3. Now we'll test the intervals between these critical points as well as points before and after the critical points. The intervals are: x &lt; -14, -14 &lt; x &lt; -3, x &gt; -3.</td> </tr> <tr> <td>LLM assistant (continued)</td> <td>Here's Python code to test the inequality for each interval: <code>from sympy import symbols; x = symbols("x"); inequality = (2*x + 10)*(x + 3) &lt; (3*x + 9)*(x + 8); test_points = [-15, -5, 0]; intervals = []; for point in test_points: if inequality.subs(x, point): intervals.append(point); intervals</code></td> </tr> <tr> <td>User Proxy Agent</td> <td>The intervals where the inequality holds true are for test points: [-15, 0]</td> </tr> <tr> <td>LLM assistant</td> <td>We have tested the intervals and found that the inequality holds for: 1. x &lt; -14 (test point: -15) 2. x &gt; -3 (test point: 0). Therefore, the solution to the inequality (2x+10)(x+3) &lt; (3x+9)(x+8) is x ∈ (-∞, -14) ∪ (-3, ∞)</td> </tr> </tbody> </table> <nav class=md-post__action> <a href=../../../../2023/06/28/MathChat/ > Continue reading </a> </nav> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://github.com/sonichi.png alt="Chi Wang"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2023-05-18 00:00:00+00:00">May 18, 2023</time></li> <li class=md-meta__item> in <a href=../../ class=md-meta__link>LLM</a>, <a href=../../../gpt/ class=md-meta__link>GPT</a>, <a href=../../../research/ class=md-meta__link>research</a></li> <li class=md-meta__item> 7 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=achieve-more-pay-less-use-gpt-4-smartly><a href=../../../../2023/05/18/GPT-adaptive-humaneval/ class=toclink>Achieve More, Pay Less - Use GPT-4 Smartly</a></h2> <p><img alt="An adaptive way of using GPT-3.5 and GPT-4 outperforms GPT-4 in both coding success rate and inference cost" src=../../../../2023-05-18-GPT-adaptive-humaneval/img/humaneval.png></p> <p><strong>TL;DR:</strong></p> <ul> <li><strong>A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.</strong></li> </ul> <p>GPT-4 is a big upgrade of foundation model capability, e.g., in code and math, accompanied by a much higher (more than 10x) price per token to use over GPT-3.5-Turbo. On a code completion benchmark, <a href=https://huggingface.co/datasets/openai_humaneval>HumanEval</a>, developed by OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%. It is possible to increase the success rate of GPT-4 further by generating multiple responses or making multiple calls. However, that will further increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and with more restricted API call rate limit. Can we achieve more with less?</p> <p>In this blog post, we will explore a creative, adaptive way of using GPT models which leads to a big leap forward.</p> <h3 id=observations><a class=toclink href=../../../../2023/05/18/GPT-adaptive-humaneval/#observations>Observations</a></h3> <ul> <li>GPT-3.5-Turbo can already solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.</li> <li>If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.</li> </ul> <p>The obstacle of leveraging these observations is that we do not know <em>a priori</em> which tasks can be solved by the cheaper model, which tasks can be solved by the expensive model, and which tasks can be solved by paying even more to the expensive model.</p> <p>To overcome that obstacle, one may want to predict which task requires what model to solve and how many responses are required for each task. Let's look at one example code completion task:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-20-1 name=__codelineno-20-1 href=#__codelineno-20-1></a><span class=k>def</span><span class=w> </span><span class=nf>vowels_count</span><span class=p>(</span><span class=n>s</span><span class=p>):</span>
<a id=__codelineno-20-2 name=__codelineno-20-2 href=#__codelineno-20-2></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Write a function vowels_count which takes a string representing</span>
<a id=__codelineno-20-3 name=__codelineno-20-3 href=#__codelineno-20-3></a><span class=sd>    a word as input and returns the number of vowels in the string.</span>
<a id=__codelineno-20-4 name=__codelineno-20-4 href=#__codelineno-20-4></a><span class=sd>    Vowels in this case are &#39;a&#39;, &#39;e&#39;, &#39;i&#39;, &#39;o&#39;, &#39;u&#39;. Here, &#39;y&#39; is also a</span>
<a id=__codelineno-20-5 name=__codelineno-20-5 href=#__codelineno-20-5></a><span class=sd>    vowel, but only when it is at the end of the given word.</span>
<a id=__codelineno-20-6 name=__codelineno-20-6 href=#__codelineno-20-6></a>
<a id=__codelineno-20-7 name=__codelineno-20-7 href=#__codelineno-20-7></a><span class=sd>    Example:</span>
<a id=__codelineno-20-8 name=__codelineno-20-8 href=#__codelineno-20-8></a><span class=sd>    &gt;&gt;&gt; vowels_count(&quot;abcde&quot;)</span>
<a id=__codelineno-20-9 name=__codelineno-20-9 href=#__codelineno-20-9></a><span class=sd>    2</span>
<a id=__codelineno-20-10 name=__codelineno-20-10 href=#__codelineno-20-10></a><span class=sd>    &gt;&gt;&gt; vowels_count(&quot;ACEDY&quot;)</span>
<a id=__codelineno-20-11 name=__codelineno-20-11 href=#__codelineno-20-11></a><span class=sd>    3</span>
<a id=__codelineno-20-12 name=__codelineno-20-12 href=#__codelineno-20-12></a><span class=sd>    &quot;&quot;&quot;</span>
</code></pre></div> <p>Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo does not consistently get it right, if we only give it one chance. It's not obvious (but an interesting research question!) how to predict the performance without actually trying.</p> <p>What else can we do? We notice that: <strong>It's "easier" to verify a given solution than finding a correct solution from scratch.</strong></p> <p>Some simple example test cases are provided in the docstr. If we already have a response generated by a model, we can use those test cases to filter wrong implementations, and either use a more powerful model or generate more responses, until the result passes the example test cases. Moreover, this step can be automated by asking GPT-3.5-Turbo to generate assertion statements from the examples given in the docstr (a simpler task where we can place our bet) and executing the code.</p> <nav class=md-post__action> <a href=../../../../2023/05/18/GPT-adaptive-humaneval/ > Continue reading </a> </nav> </div> </article> <article class="md-post md-post--excerpt"> <header class=md-post__header> <nav class="md-post__authors md-typeset"> <span class=md-author> <img src=https://github.com/sonichi.png alt="Chi Wang"> </span> </nav> <div class="md-post__meta md-meta"> <ul class=md-meta__list> <li class=md-meta__item> <time datetime="2023-04-21 00:00:00+00:00">April 21, 2023</time></li> <li class=md-meta__item> in <a href=../../ class=md-meta__link>LLM</a>, <a href=../../../gpt/ class=md-meta__link>GPT</a>, <a href=../../../research/ class=md-meta__link>research</a></li> <li class=md-meta__item> 5 min read </li> </ul> </div> </header> <div class="md-post__content md-typeset"> <h2 id=does-model-and-inference-parameter-matter-in-llm-applications-a-case-study-for-math><a href=../../../../2023/04/21/LLM-tuning-math/ class=toclink>Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH</a></h2> <p><img alt="level 2 algebra" src=../../../../2023-04-21-LLM-tuning-math/img/level2algebra.png></p> <p><strong>TL;DR:</strong> * <strong>Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.</strong> * <strong>For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.</strong> * <strong>AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.</strong></p> <p>Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?</p> <p>In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for <a href=https://github.com/hendrycks/math>MATH</a>, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.</p> <p>We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search &amp; pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.</p> <p>We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.</p> <h3 id=experiment-setup><a class=toclink href=../../../../2023/04/21/LLM-tuning-math/#experiment-setup>Experiment Setup</a></h3> <p>We use AutoGen to select between the following models with a target inference budget $0.02 per instance: - gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app - gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo</p> <p>We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:</p> <ul> <li>temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of [0, 1].</li> <li>top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of [0, 1].</li> <li>max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of [50, 1000].</li> <li>n: The number of responses to generate. We search for the optimal n in the range of [1, 100].</li> <li>prompt: We use the template: "{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \boxed{{}}." where {problem} will be replaced by the math problem instance.</li> </ul> <p>In this experiment, when n &gt; 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.</p> <nav class=md-post__action> <a href=../../../../2023/04/21/LLM-tuning-math/ > Continue reading </a> </nav> </div> </article> <nav class=md-pagination> <a href=../../ class=md-pagination__link>1</a> <span class=md-pagination__current>2</span> </nav> </div> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> &copy; 2025 <a href=https://ag2.ai/ target=_blank rel=noopener>ag2</a> </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://discord.gg/pAbnFJrkgZ target=_blank rel=noopener title=discord.gg class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485 485 0 0 0 404.081 32.03a1.82 1.82 0 0 0-1.923.91 338 338 0 0 0-14.9 30.6 447.9 447.9 0 0 0-134.426 0 310 310 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.7 483.7 0 0 0-119.688 37.107 1.7 1.7 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.02 2.02 0 0 0 .765 1.375 487.7 487.7 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348 348 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321 321 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251 251 0 0 0 9.109-7.137 1.82 1.82 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.81 1.81 0 0 1 1.924.233 235 235 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.4 301.4 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391 391 0 0 0 30.014 48.815 1.86 1.86 0 0 0 2.063.7A486 486 0 0 0 610.7 405.729a1.88 1.88 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541M222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241m195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241"/></svg> </a> <a href=https://github.com/ag2ai/ag2 target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 480 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"/></svg> </a> <a href=https://x.com/ag2oss target=_blank rel=noopener title=x.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8l164.9-188.5L26.8 48h145.6l100.5 132.9zm-24.8 373.8h39.1L151.1 88h-42z"/></svg> </a> <a href=https://www.youtube.com/@ag2ai target=_blank rel=noopener title=www.youtube.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305m-317.51 213.508V175.185l142.739 81.205z"/></svg> </a> <a href=https://www.linkedin.com/company/ag2ai target=_blank rel=noopener title=www.linkedin.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../../../../..", "features": ["search.suggest", "search.highlight", "navigation.tabs", "navigation.tabs.sticky", "navigation.indexes", "navigation.tracking", "navigation.prune", "navigation.top", "navigation.footer", "content.tabs.link", "content.code.copy", "content.code.annotate", "content.action.edit"], "search": "../../../../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script> <script src=../../../../../../assets/javascripts/bundle.c8b220af.min.js></script> <script src=../../../../../../js/timeago.min.js></script> <script src=../../../../../../js/timeago_mkdocs_material.js></script> <script src=../../../../../../javascripts/extra.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>